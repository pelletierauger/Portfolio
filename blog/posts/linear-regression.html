<!-- year -->2019
<!-- month -->01
<!-- day -->26
<!-- fr-title -->Régression linéaire
<!-- fr-description -->Quelques pas vers l'apprentissage automatique.
<!-- fr-content -->

<pn>
    <sc>J’ai rassemblé ci-dessous</sc> des notes que j'ai prises en suivant <a href="https://www.youtube.com/watch?v=XJ7HLz9VYz0&list=PLRqwX-V7Uu6Y7MdSCaIfsxc561QI0U0Tb">les tutoriels vidéo de Daniel Shiffman sur les réseaux neuronaux</a>, qui eux-mêmes s'inspirent largement de <i>Make Your Own Neural Network</i>, un livre écrit par Tariq Rashid. Les concepts et les formules ne sont pas de moi, je ne fais que les écrire afin de m'aider à les comprendre et à les mémoriser.
</pn>

<h3>Pente et ordonnée à l'origine</h3>

<pn>
    <sc>Une droite</sc> est traditionnellement représentée par cette équation, dans laquelle <im>m</im> représente la pente et <im>b</im> l'ordonnée à l'origine :
</pn>
<math>
    y = mx + b
</math>
<pn>
    Dans le cas de la régression linéaire (ou <a href="https://fr.wikipedia.org/wiki/M%C3%A9thode_des_moindres_carr%C3%A9s_ordinaire">méthode des moindres carrés ordinaire</a>), nous allons calculer la valeur <im>m</im> avec la formule &nbsn;ci-dessous.<snl label="linear-video">Cette formule est tirée de <a href="https://www.youtube.com/watch?v=szXbuO3bVRk">cette vidéo</a> réalisée par Daniel Shiffman dans le cadre de son cours <i8>Intelligence et apprentissage</i8>.</snl> Nous pouvons considérer que le numérateur de cette fraction représente la corrélation entre la croissance de la valeur <im>x</im> et celle de la valeur <im>y</im> (qui détermine la pente de notre droite).
</pn>
<div class="mathbox-clear-left">
    <math>
m = \frac{\sum\limits_{i=0}^n(x_i-\bar{x})(y_i-\bar{y})}{\sum\limits_{i=0}^n(x_i-\bar{x})^2}
</math>
</div>
<!-- read-more -->
<pn>
    Ici, <im>\bar{x}</im> représente la moyenne de toutes les valeurs <im>x</im>, donc la somme de tous les <im>x</im> divisée par la quantité <im>n</im> de ces valeurs.
</pn>

<math>
\bar{x} = \frac{\sum\limits_{i=0}^n x_i}{n}
</math>

<pn>
    Quant à <im>b</im>, l'ordonnée à l'origine, nous la calculerons ainsi :
</pn>
<math>
    b = \bar{y} - m\bar{x}
</math>


<h3>Contexte</h3>

<pn>
    <sc>Cette note de blog</sc> fait partie de mon projet de recherche <i>Vers un cinéma algorithmique</i>, démarré en avril 2018. Je vous invite à consulter <a href="https://pelletierauger.github.io/fr/blog/2018/4/vers-un-cinema-algorithmique.html">la toute première note du projet</a> pour en apprendre davantage.
</pn>

<!-- en-title -->Linear regression
<!-- en-description -->Small steps towards machine learning.
<!-- en-content -->
<pn>
    <sc>Below are some notes</sc> that I took while following <a href="https://www.youtube.com/watch?v=XJ7HLz9VYz0&list=PLRqwX-V7Uu6Y7MdSCaIfsxc561QI0U0Tb">Daniel Shiffman's video tutorials on neural networks</a>, which are heavily inspired by <i>Make Your Own Neural Network</i>, a book written by Tariq Rashid. The concepts and formulas here are not my original material, I just wrote them down in order to better understand and remember them.
</pn>

<h3>Slope and intercept</h3>

<pn>
    <sc>A line</sc> is traditionally represented by this equation, in which <im>m</im> represents the slope and <im>b</im> the <im>y</im> intercept at the origin:
</pn>
<math>
    y = mx + b
</math>
<pn>
    In the case of linear regression (or <a href="https://en.wikipedia.org/wiki/Ordinary_least_squares"> the ordinary least squares method</a>), we will calculate the <im>m</im> value with the formula &nbsn;below.<snl label="linear-video">This formula is taken from <a href="https://www.youtube.com/watch?v=szXbuO3bVRk">this video</a> made by Daniel Shiffman for his <i8>Intelligence and Learning</i8> course.</snl> We will consider that the numerator of this fraction represents the correlation between the growth of the value <im>x</im> and the growth of the value <im>y</im> (which determines the slope of the line).
</pn>
<div class="mathbox-clear-left">
    <math>
m = \frac{\sum\limits_{i=0}^n(x_i-\bar{x})(y_i-\bar{y})}{\sum\limits_{i=0}^n(x_i-\bar{x})^2}
</math>
</div>
<!-- read-more -->
<pn>
    Here, <im>\bar{x}</im> represents the average of all the <im>x</im> values, which is the sum of all the <im>x</im> divided by the amount <im>n</im> of those values.
</pn>

<math>
\bar{x} = \frac{\sum\limits_{i=0}^n x_i}{n}
</math>

<pn>
    As for <im>b</im>, the <im>y</im> intercept, we will calculate it thusly:
</pn>
<math>
    b = \bar{y} - m\bar{x}
</math>

<h3>Context</h3>
<pn>
    <sc>This blog post</sc> is part of my research project <i>Towards an algorithmic cinema</i>, started in April 2018. I invite you to read <a href="https://pelletierauger.github.io/en/blog/2018/4/towards-an-algorithmic-cinema.html">the first blog post of the project</a> to learn more about it.
</pn>